from fastapi import APIRouter, File, UploadFile, HTTPException
from fastapi.responses import JSONResponse, StreamingResponse
from pydantic import BaseModel
from openai import OpenAI
import os
import httpx
import json
import base64
from io import BytesIO
from PIL import Image


router = APIRouter()

# Récupération des variables d'environnement
OLLAMA_API_URL = os.getenv("OLLAMA_API_URL")
LLM_MODELS = os.getenv("LLM_MODELS")
VLM_MODEL = os.getenv("VLM_MODEL")

# Initialize OpenAI client
openai_client = OpenAI(
    base_url=f"{OLLAMA_API_URL}/v1/",
    api_key="ollama",  # Required but ignored for OpenAI compatibility
)


# Modèle pour recevoir le message utilisateur
class ChatRequest(BaseModel):
    prompt: str


async def stream_response(payload: dict):
    """
    Fonction asynchrone pour streamer les réponses de l'API `/api/generate`.
    """
    async with httpx.AsyncClient() as client:
        async with client.stream(
            "POST", f"{OLLAMA_API_URL}/api/generate", json=payload
        ) as response:
            async for line in response.aiter_lines():
                if line.strip():  # Ignore les lignes vides
                    chunk = json.loads(line)
                    if "response" in chunk:
                        yield chunk["response"]


@router.post("/api/chat")
async def chat_endpoint(request: ChatRequest, model: str):
    """Stream the LLM response with the selected model."""
    # Vérifiez que le modèle est valide
    available_models = LLM_MODELS.split(",")
    if model not in available_models:
        raise HTTPException(
            status_code=400,
            detail=f"Model '{model}' is not available. Choose from {available_models}.",
        )

    payload = {"model": model, "prompt": request.prompt}

    try:
        return StreamingResponse(stream_response(payload), media_type="text/plain")
    except Exception as e:
        raise HTTPException(
            status_code=500, detail=f"Erreur lors de la communication avec Ollama: {e}"
        )


@router.get("/api/models")
async def get_models():
    """Retourne la liste des modèles disponibles."""
    models = LLM_MODELS.split(",")
    return {"models": models}


@router.post("/api/image-description")
async def image_description(image: UploadFile = File(...)):
    """
    Process an image and generate a description using OpenAI's model.
    """
    try:
        # Read the image file
        image_data = await image.read()
        print(f"Image received: {image.filename}, size: {len(image_data)} bytes")

        # Open and verify the image
        img = Image.open(BytesIO(image_data))
        img.verify()  # Verify image integrity
        print(f"Image verified: {img.format}, size: {img.size}")

        # Convert image to base64
        img_base64 = base64.b64encode(image_data).decode("utf-8")
        img_url = f"data:{image.content_type};base64,{img_base64}"
        print(f"Base64 Image URL generated (truncated): {img_url[:100]}...")

        # Create the prompt with the base64 image
        response = openai_client.chat.completions.create(
            model="llava",
            messages=[
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": "What's in this image?"},
                        {"type": "image_url", "image_url": img_url},
                    ],
                }
            ],
            max_tokens=300,
        )
        print(f"OpenAI API Response: {response}")

        # Extract and return the generated description
        if response.choices and response.choices[0].message:
            description = response.choices[0].message.content.strip()
            print(f"Generated Description: {description}")
            return JSONResponse(content={"description": description})
        else:
            raise HTTPException(
                status_code=500, detail="No valid response generated by OpenAI."
            )

    except Exception as e:
        print(f"Error processing image: {str(e)}")
        raise HTTPException(
            status_code=500, detail=f"Erreur lors du traitement de l'image : {str(e)}"
        )
